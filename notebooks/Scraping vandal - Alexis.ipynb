{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f91f089",
   "metadata": {},
   "source": [
    "# PyVG: Data Science to predict Video Games sales\n",
    ">Equipe: Alexis Terrasse, Henri-François Mole, Hsan Drissi, Stephane Lelievre\n",
    ">\n",
    ">Promo: DS_Oct21\n",
    "---\n",
    "## Scraping Vandal - Alexis\n",
    "\n",
    "<img src='https://media.vandalimg.com/common/1200x800/2016422115313.jpg' width=500></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c9af2",
   "metadata": {},
   "source": [
    "### Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Importation de BeautifulSoup\n",
    "import requests # Importation de import requests \n",
    "from time import sleep # Importation de sleep\n",
    "import pandas as pd # Importation de pandas sous l'alias pd\n",
    "import numpy as np # Importation de numpy sous l'alias np\n",
    "from tqdm import tqdm # Importation de tqdm\n",
    "import datetime # Importation de datetime\n",
    "import os # Importation de os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13233e0",
   "metadata": {},
   "source": [
    "### Définition de fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40324006",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "######### Fonction de récupération des données du jeu  ###########\n",
    "##################################################################\n",
    "\n",
    "def retrieve_data(sp, plat): \n",
    "        \n",
    "    platf= '\\''+plat.lower()+'\\''\n",
    "        \n",
    "    g_name= sp.select('.linkversion')[0].text.strip() # On récupère le titre du jeu \n",
    "    \n",
    "    s_sp= sp.select('#sombreado')[0]\n",
    "    if s_sp.select('li[class=\\\"t12 fnormal\\\"]') != [] :   \n",
    "        g_release= s_sp.select('li[class=\\\"t12 fnormal\\\"]')[0].text[22:]\n",
    "    else : g_release = None\n",
    "        \n",
    "    g_analisis= np.nan\n",
    "    for item in sp.select('.linktotal'):\n",
    "        item_str= str(item)\n",
    "        if platf in item_str :\n",
    "            g_analisis= item.text\n",
    "            \n",
    "    g_comunidad=np.nan\n",
    "    for item in sp.select('.notacomunidad'):\n",
    "        if item.select('span[class=falsolink]') != []:\n",
    "            if platf in item.select('span[class=falsolink]')[0].attrs['data-ruta'] :\n",
    "                g_comunidad= item.text\n",
    "    \n",
    "    g_nvotes= 0    \n",
    "    for item in sp.select('.votoscomunidad'):\n",
    "        if item.select('span') != []:             \n",
    "            platf= '\\''+plat.lower()+'\\''\n",
    "            if platf in item.select('span')[0].attrs['data-ruta']:\n",
    "                if len(item.text) > 6:\n",
    "                    g_nvotes= item.text[:-6]\n",
    "\n",
    "    if sp.select('.span8 div div div .mt05 b') == []: \n",
    "        g_name_en= g_name\n",
    "    else : \n",
    "        g_name_en= sp.select('.span8 div div div .mt05 b')[0].text.strip() \n",
    "\n",
    "    return(g_name, g_release, g_analisis, g_comunidad, g_nvotes, g_name_en)\n",
    "\n",
    "\n",
    "##################################################################\n",
    "###### Fonction de parcours des jeux de la page active    ########\n",
    "##################################################################\n",
    "\n",
    "def scrap_page_title(url):\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    response  = requests.get(url, headers = user_agent, timeout=60) # On requête le serveur\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') #On parse le résultat avec BeautifulSoup\n",
    "    \n",
    "    for link in soup.select('.t11 div'):\n",
    "\n",
    "            plat= link.select('span')[0].text\n",
    "            \n",
    "\n",
    "            game_url= 'https://vandal.elespanol.com'+link.select('a')[0].attrs['href']\n",
    "            user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "            response  = requests.get(game_url, headers = user_agent, timeout=60) # On requête le serveur\n",
    "            soup = BeautifulSoup(response.text, 'html.parser') #On parse le résultat avec BeautifulSoup\n",
    "            if game_url in broken_link:\n",
    "                continue\n",
    "            else :\n",
    "                platform.append(plat)  \n",
    "                \n",
    "                print(game_url)\n",
    "\n",
    "                p_name, p_release, p_analisis, p_comunidad, p_votes, p_name_en= retrieve_data(soup, plat)\n",
    "\n",
    "                gname.append(p_name)\n",
    "                \n",
    "                release.append(p_release)\n",
    "\n",
    "                analisis.append(p_analisis)\n",
    "\n",
    "                comunidad.append(p_comunidad)\n",
    "\n",
    "                n_votes.append(p_votes)\n",
    "\n",
    "                name_en.append(p_name_en)     \n",
    "\n",
    "                print('Name:',p_name,' -')\n",
    "                \n",
    "\n",
    "##################################################################\n",
    "######### Fonction de parcours des pages par années   ############\n",
    "##################################################################\n",
    "                \n",
    "def vandal_scrap(start_y,end_y):\n",
    "    \n",
    "    dict_annee={2017: 77, 2018: 758, 2019: 860, 2020: 865, 2021: 812, 2022: 139}\n",
    "    for annee in range(start_y,end_y):\n",
    "        print('année:', annee)\n",
    "        current_page= 0\n",
    "        while current_page != -1 :        \n",
    "            url= url_start+str(current_page)+'/'+str(annee)\n",
    "            headermap = {\"User-Agent\": \"Mac Firefox\"};\n",
    "            markup = requests.get(url, headers=headermap).text\n",
    "            soup= BeautifulSoup(markup,\"lxml\")                   \n",
    "            \n",
    "            if annee >= 2017 :\n",
    "                if current_page+100 > dict_annee[annee]:\n",
    "                    current_page = -1\n",
    "                else: current_page+= 100\n",
    "            else : \n",
    "                if (soup.select('.pildora')[0].text == 'Anteriores 100 puestos') and (len(soup.select('.pildora')) == 1):\n",
    "                    current_page = -1\n",
    "                else : current_page+= 100\n",
    "            \n",
    "            scrap_page_title(url)\n",
    "        \n",
    "        columns = {\n",
    "            'ES_Name': gname,\n",
    "            'Name': name_en,\n",
    "            'Release_date': release,\n",
    "            'platform': platform,\n",
    "            'Analyst_Score': analisis,\n",
    "            'Community_score': comunidad,\n",
    "            'N_com_votes': n_votes,\n",
    "        }\n",
    "    \n",
    "        df = pd.DataFrame(columns)\n",
    "\n",
    "        file_name='vandal_'+str(annee)+'.csv'\n",
    "\n",
    "        df.to_csv(file_name, sep=\",\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8313ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_page_title(url):\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    response  = requests.get(url, headers = user_agent, timeout=60) # On requête le serveur\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') #On parse le résultat avec BeautifulSoup\n",
    "    \n",
    "    for link in soup.select('.t11 div'):\n",
    "\n",
    "            plat= link.select('span')[0].text\n",
    "            \n",
    "\n",
    "            game_url= 'https://vandal.elespanol.com'+link.select('a')[0].attrs['href']\n",
    "            user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "            response  = requests.get(game_url, headers = user_agent, timeout=60) # On requête le serveur\n",
    "            soup = BeautifulSoup(response.text, 'html.parser') #On parse le résultat avec BeautifulSoup\n",
    "            if game_url in broken_link:\n",
    "                continue\n",
    "            else :\n",
    "                platform.append(plat)  \n",
    "                \n",
    "                print(game_url)\n",
    "\n",
    "                p_name, p_release, p_analisis, p_comunidad, p_votes, p_name_en= retrieve_data(soup, plat)\n",
    "\n",
    "                gname.append(p_name)\n",
    "                \n",
    "                release.append(p_release)\n",
    "\n",
    "                analisis.append(p_analisis)\n",
    "\n",
    "                comunidad.append(p_comunidad)\n",
    "\n",
    "                n_votes.append(p_votes)\n",
    "\n",
    "                name_en.append(p_name_en)     \n",
    "\n",
    "                print('Name:',p_name,' -')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0385b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# début de l'url du site trié par metascore et par année\n",
    "url_start = 'https://vandal.elespanol.com/rankings/videojuegos/todos/vandal/' # début de l'url du site trié par metascore et par année\n",
    "\n",
    "# Lien hs\n",
    "broken_link = ['https://vandal.elespanol.com/juegos/pc/7th-sector/70512#p-83', \n",
    "               'https://vandal.elespanol.com/juegos/pc/7th-sector/70512#p-79',\n",
    "               'https://vandal.elespanol.com/juegos/pc/7th-sector/70512#p-73',\n",
    "               'https://vandal.elespanol.com/juegos/pc/7th-sector/70512#p-13',\n",
    "              ]\n",
    "\n",
    "    # Fonction (main) du scraping de metacritic\n",
    "\n",
    "gname = []\n",
    "release= []\n",
    "platform =[]\n",
    "analisis = []\n",
    "comunidad = []\n",
    "n_votes= []\n",
    "name_en = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91499287",
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDateTime = datetime.datetime.now()\n",
    "date = currentDateTime.date()\n",
    "current_year = date.strftime(\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping des données de 2003 à année actuelle (si le csv n'existe pas déjà)\n",
    "for i in range(2003, int(current_year)+1):\n",
    "    vandal_year= './vandal_'+str(i)+'.csv'\n",
    "    if os.path.exists(vandal_year) == False :\n",
    "        vandal_scrap(i,i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regroupement des différents csv\n",
    "vandal= pd.DataFrame()\n",
    "for i in range(2003,2023): # Pas de données avant 1995\n",
    "    filename= 'vandal_'+str(i)+'.csv'\n",
    "    df= pd.read_csv(filename)\n",
    "    vandal= pd.concat([vandal,df],ignore_index=True)\n",
    "\n",
    "vandal= vandal.drop_duplicates(keep='first')\n",
    "vandal.to_csv('vandal.csv', sep=\",\", encoding='utf-8', index=False)\n",
    "vandal.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
