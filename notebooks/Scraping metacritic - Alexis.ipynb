{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55284bfa",
   "metadata": {},
   "source": [
    "# PyVG: Data Science to predict Video Games sales\n",
    ">Equipe: Alexis Terrasse, Henri-François Mole, Hsan Drissi, Stephane Lelievre\n",
    ">\n",
    ">Promo: DS_Oct21\n",
    "---\n",
    "## Scraping Metacritic - Alexis\n",
    "<img src='https://earlygame.com/uploads/images/_800x418_crop_center-center_82_none/metacritic.jpg?mtime=1591260456' width=500></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674ee0a",
   "metadata": {},
   "source": [
    "### Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7aec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Importation de BeautifulSoup\n",
    "import requests # Importation de import requests \n",
    "from time import sleep # Importation de sleep\n",
    "import pandas as pd # Importation de pandas sous l'alias pd\n",
    "from tqdm import tqdm # Importation de tqdm\n",
    "import datetime # Importation de datetime\n",
    "import os # Importation de os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b5f072",
   "metadata": {},
   "source": [
    "### Définition de fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ffc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "######### Fonction de récupération des données ddu jeu  ##########\n",
    "##################################################################\n",
    "\n",
    "def retrieve_data(sp): \n",
    "    \n",
    "    # Fonction de récupération du Nom, de la plateforme, de l'éditeur, \n",
    "    # de l'année de sortie, du/des genres, du metascore et du userscore \n",
    "            \n",
    "    span= sp.select(\"span[class=data]\") # On récupère les balises span contenant class=data dans une liste\n",
    "    \n",
    "    g_name= sp.select('h1')[0].text.strip() # On récupère le titre du jeu \n",
    "    \n",
    "    g_platform= sp.select('.platform')[0].text.strip() # On récupère la plateforme\n",
    "\n",
    "    # On récupère l'éditeur \n",
    "    if sp.select('.publisher a') != [] :\n",
    "        g_publisher= sp.select('.publisher a')[0].text.strip()\n",
    "    else :\n",
    "        g_publisher=None\n",
    "      \n",
    "    # On récupère l'année de sortie\n",
    "    if sp.select('.release_data .data') != [] :\n",
    "        g_year=sp.select('.release_data .data')[0].text.strip()\n",
    "    else :\n",
    "        g_year=None    \n",
    "        \n",
    "    # On récupère le développeur lorsqu'il est dispo\n",
    "    if sp.select('.developer .label') != [] :\n",
    "        g_developer=sp.select('.button')[0].text.strip()\n",
    "    else :\n",
    "        g_developer=None\n",
    "        \n",
    "    # Pour le genre comme il peut y en avoir plusieurs on les concatène après les avoir récupèré\n",
    "    g_genre= ', '.join(sp.find_all('li',{'class' : 'summary_detail product_genre'})[0].text[9:].strip().split(',                                            '))\n",
    "    \n",
    "    \n",
    "    # Pour le metascore on vérifie s'il la valeur existe si ce n'est pas le cas alors g_metas reçoit None\n",
    "    g_metas= -1\n",
    "    for tag in sp.select('span',{'class':'desc'}):\n",
    "        if 'No score yet' in tag.text:\n",
    "            g_metas= None\n",
    "    if g_metas != None :\n",
    "        g_metas= int(sp.select(\"a[class^=metascore_anchor]\")[0].text) #Sinon on récupère le metascore\n",
    "    \n",
    "    # Pour le userscore on vérifie s'il reste à déterminer 'tbd'\n",
    "    if sp.select(\"a[class^=metascore_anchor]\")[1].text.strip() == 'tbd':\n",
    "        g_users= None #Si oui alors on lui affecte Non\n",
    "    else : #Sinon s'il n'y a pas de g_metas on affecte la valeur du premier élément de la liste sp.select(\"a[class^=metascore_anchor]\")\n",
    "        if g_metas == None: g_users= float(sp.select(\"a[class^=metascore_anchor]\")[0].text)\n",
    "        else : g_users= float(sp.select(\"a[class^=metascore_anchor]\")[1].text)  #Sinon on lui affecte le second élémen\n",
    "            \n",
    "            \n",
    "    if sp.select('.count a span') != [] :\n",
    "        g_n_pro_crit= int(sp.select('.count a span')[0].text.strip())\n",
    "    else :\n",
    "        g_n_pro_crit= 0          \n",
    "            \n",
    "    if sp.select('.feature_userscore a') != [] :\n",
    "        if len(sp.select('.feature_userscore a')) > 1:\n",
    "            g_n_user_crit= int(sp.select('.feature_userscore a')[1].text.strip()[:-8])\n",
    "        else : g_n_user_crit= 0\n",
    "    else :\n",
    "        g_n_user_crit= 0           \n",
    "            \n",
    "            \n",
    "    #On retroune les différentes valeurs trouvées\n",
    "    return(g_name,g_platform,g_publisher,g_year,g_developer,g_genre,g_metas,g_users,g_n_pro_crit,g_n_user_crit ) \n",
    "\n",
    "\n",
    "##################################################################\n",
    "########## Fonction de récupération des critiques pro  ###########\n",
    "##################################################################\n",
    "\n",
    "def retrieve_pro_critics(url):     \n",
    "    # Fonction de récupération des meta reviews (critiques pro)\n",
    "    \n",
    "    crit_url= url+'/critic-reviews' # on se place sur la page des critques pro\n",
    "    headermap = {\"User-Agent\": \"Mac Firefox\"};\n",
    "    crit_markup = requests.get(crit_url, headers=headermap, timeout=60).text # On requête le serveur\n",
    "    crit_soup= BeautifulSoup(crit_markup,\"lxml\") #On parse le résultat avec BeautifulSoup\n",
    "    \n",
    "    #On détermine le nombre de pages pour les reviews pro\n",
    "    if len(crit_soup.select(\"ul[class=pages]\")) == 0 : \n",
    "        crit_pages = 0\n",
    "    else : \n",
    "        crit_pages= int(crit_soup.select(\"ul[class=pages]\")[0].select(\"a[class=page_num]\")[-1].text)\n",
    "    \n",
    "    #On instancie une liste vide dans laquelle on stockera les infos de chaque reviews\n",
    "    crit_review=[]   \n",
    "    \n",
    "    if crit_pages == 0 : # S'il n'y a qu'une page \n",
    "        \n",
    "        # On parcours toutes les critiques de la page (trouvées dans les balises crit_soup.select(\"div[class^=review_content]\")[:-3])\n",
    "        for review in crit_soup.select(\"div[class^=review_content]\")[:-3] :\n",
    "            if review.find('div', class_='name') == None: break # Si on trouve rien on s'arrête\n",
    "                \n",
    "            #Récupération de la date de la review\n",
    "            if review.select(\"div[class^=date]\") != [] : #Si on trouve une balise avec la date \n",
    "                review_date= review.select(\"div[class^=date]\")[0].text # On l'affecte à la variable review_date\n",
    "            else : review_date= 'None' # Sinon on affecte None à review_date\n",
    "                \n",
    "            review_name= review.select(\"div[class^=source]\")[0].text # Récupération du nom du critique\n",
    "            review_score= review.select(\"div[class^=metascore_w]\")[0].text # Récupération du score donné par le critique\n",
    "            review_body= review.select(\"div[class^=review_body]\")[0].text.strip() # Récupération du commentaire du critique\n",
    "            crit_review.append([review_date,review_name,review_score,review_body]) # On ajoute les infos à la liste crit_review\n",
    "            \n",
    "    else:  \n",
    "        # S'il y a plusieurs pages on répète l'opération ci-dessous mais pour chaque page\n",
    "        for i in range(0,len(crit_pages)+1):\n",
    "            \n",
    "            crit_url= url+'/critic-reviews&page='+str(i)\n",
    "            headermap = {\"User-Agent\": \"Mac Firefox\"};\n",
    "            crit_markup = requests.get(crit_url, headers=headermap,timeout=60).text\n",
    "            crit_soup= BeautifulSoup(markup,\"lxml\")\n",
    "            \n",
    "            for review in crit_soup.select(\"div[class^=review_content]\")[:-3] :\n",
    "                if review.select(\"div[class^=date]\") != []:             \n",
    "                    review_date= review.select(\"div[class^=date]\")[0].text\n",
    "                else : review_date= 'None'\n",
    "                review_name= review.select(\"div[class^=source]\")[0].text\n",
    "                review_score= review.select(\"div[class^=metascore_w]\")[0].text\n",
    "                review_body= review.select(\"div[class^=review_body]\")[0].text.strip()\n",
    "                crit_review.append([review_date,review_name,review_score,review_body])\n",
    "                \n",
    "    return crit_review # On retourne la liste crit_review avec toutes les infos trouvées\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "##################################################################\n",
    "########## Fonction de récupération des critiques pro  ###########\n",
    "##################################################################\n",
    "\n",
    "def critic_reviews(url):\n",
    "    # Fonction de récupération des meta reviews (critiques pro)\n",
    "     \n",
    "    url= url+'/critic-reviews'  # on se place sur la page des critques pro\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    response  = requests.get(url, headers = user_agent, timeout=60) # On requête le serveur\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') #On parse le résultat avec BeautifulSoup\n",
    "    \n",
    "    #On détermine le nombre de pages pour les reviews pro\n",
    "    if len(soup.select(\"ul[class=pages]\")) == 0 : \n",
    "        max_pages = 1\n",
    "    else : \n",
    "        max_pages= int(soup.select(\"ul[class=pages]\")[0].select(\"a[class=page_num]\")[-1].text)\n",
    "\n",
    "    #On instancie une liste vide dans laquelle on stockera les infos de chaque reviews    \n",
    "    crit_review=[] \n",
    "    \n",
    "    for page in range(0,max_pages): # On parcours les pages des reviews     \n",
    "        if page > 0: url= url+'&page='+str(page) # S'il y en a plusieurs on met à jour l'url\n",
    "        user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "        response  = requests.get(url, headers = user_agent, timeout=60) # On requête le serveur\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') #On parse le résultat avec BeautifulSoup\n",
    "        \n",
    "        # On parcours toutes les critiques de la page \n",
    "        for review in soup.find_all('div', class_='review_content'):\n",
    "            \n",
    "            if review.find('div', class_='source') == None: break # Si on ne trouve rien on s'arrête\n",
    "            \n",
    "            #Récupération de la date de la review\n",
    "            if review.select(\"div[class^=date]\") != []: #Si on trouve une balise avec la date \n",
    "                review_date=review.select(\"div[class^=date]\")[0].text # On l'affecte à la variable review_date\n",
    "            else : review_date='None' # Sinon on affecte None à review_date\n",
    "                            \n",
    "            review_name=review.select(\"div[class^=source]\")[0].text.strip('\\n')  # Récupération du nom du critique\n",
    "\n",
    "            review_score=review.select(\"div[class^=metascore_w]\")[0].text  # Récupération du score donné par le critique\n",
    "            \n",
    "            # Récupération du commentaire du critique\n",
    "            if review.find('span', class_='blurb blurb_expanded'):\n",
    "                review_body=review.find('span', class_='blurb blurb_expanded').text\n",
    "            else:\n",
    "                review_body=review.select(\"div[class^=review_body]\")[0].text.strip()\n",
    "                \n",
    "            crit_review.append([review_date,review_name,review_score,review_body]) # On ajoute les infos à la liste crit_review\n",
    "\n",
    "    return crit_review # On retourne la liste crit_review avec toutes reviews de la/les pages\n",
    "\n",
    "\n",
    "##################################################################\n",
    "########## Fonction de récupération des critiques pro  ###########\n",
    "##################################################################\n",
    "\n",
    "def user_reviews(url):  \n",
    "    # Fonction de récupération des meta reviews (critiques utilisateurs) \n",
    "   \n",
    "    url= url+'/user-reviews'  # on se place sur la page des critques users\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    response  = requests.get(url, headers = user_agent, timeout=60) # On requête le serveur\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') #On parse le résultat avec BeautifulSoup\n",
    "    \n",
    "    #On détermine le nombre de pages pour les reviews pro\n",
    "    if len(soup.select(\"ul[class=pages]\")) == 0 : \n",
    "        max_pages = 1\n",
    "    else : \n",
    "        max_pages= int(soup.select(\"ul[class=pages]\")[0].select(\"a[class=page_num]\")[-1].text) \n",
    "    \n",
    "    #On instancie une liste vide dans laquelle on stockera les infos de chaque reviews \n",
    "    user_review=[]\n",
    "\n",
    "    for page in range(0,max_pages): # On parcours les pages des reviews \n",
    "        url= url+'?sort-by=score&num_items=100&page='+str(page) # On se place sur l'urlen fonction à sa page\n",
    "        user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "        response  = requests.get(url, headers = user_agent, timeout=60) # On requête le serveur\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') #On parse le résultat avec BeautifulSoup\n",
    "        \n",
    "        # On parcours toutes les critiques de la page\n",
    "        for review in soup.find_all('div', class_='review_content'):\n",
    "            #Récupération de la date de la review\n",
    "            if review.find('div', class_='name') == None: break # Si on ne trouve rien on s'arrête\n",
    "\n",
    "            if review.select(\"div[class^=date]\") != []:#Si on trouve une balise avec la date\n",
    "                review_date=review.select(\"div[class^=date]\")[0].text # On l'affecte à la variable review_date\n",
    "            else : review_date='None' # Sinon on affecte None à review_date\n",
    "                            \n",
    "            review_name=review.select(\"div[class^=name]\")[0].text.strip('\\n') # Récupération du nom du user\n",
    "            \n",
    "            review_score=review.select(\"div[class^=metascore_w]\")[0].text  # Récupération du score donné par le user\n",
    "            \n",
    "            # Récupération du commentaire du critique\n",
    "            if review.find('span', class_='blurb blurb_expanded'):\n",
    "                review_body=review.find('span', class_='blurb blurb_expanded').text\n",
    "            else:\n",
    "                review_body=review.select(\"div[class^=review_body]\")[0].text.strip()\n",
    "                \n",
    "            user_review.append([review_date,review_name,review_score,review_body]) # On ajoute les infos à la liste user_review\n",
    "\n",
    "    return user_review # On retourne la liste user_review avec toutes reviews de la/les pages\n",
    "\n",
    "\n",
    "##################################################################\n",
    "####### Fonction de parcours des jeu sur la page active  #########\n",
    "##################################################################\n",
    "\n",
    "def scrap_page_title(url,start_page,start_title,nb_p):\n",
    "    # Fonction de scraping de la page de recherche de résultat \n",
    "\n",
    "    for p in range(start_page,nb_p):\n",
    "        print(p)\n",
    "        url= url+'&page='+str(p)\n",
    "        headermap = {\"User-Agent\": \"Mac Firefox\"};\n",
    "        markup = requests.get(url, headers=headermap).text\n",
    "        soup= BeautifulSoup(markup,\"lxml\")\n",
    "        \n",
    "\n",
    "        games_list= soup.select('td',{'class' : 'clamp-image-wrap'})\n",
    "        games_list=[elt for idx, elt in enumerate(games_list) if idx % 2 == 0]\n",
    "\n",
    "        for game in games_list[start_title:]:   \n",
    "            game_url= 'https://www.metacritic.com'+game.find('a',href=True).attrs['href']\n",
    "            #games_url.append(game_url)\n",
    "            print(game_url)\n",
    "            if game_url in broken_link:\n",
    "                continue\n",
    "            else :\n",
    "                headermap = {\"User-Agent\": \"Mac Firefox\"};\n",
    "                game_markup = requests.get(game_url, headers=headermap).text\n",
    "                sleep(2)\n",
    "                game_soup= BeautifulSoup(game_markup,\"lxml\")                \n",
    "                \n",
    "                p_name,p_platform,p_publisher,p_year,p_developer,p_genre,p_metascore,p_userscore,p_n_pro, p_n_user= retrieve_data(game_soup)\n",
    "                \n",
    "                gname.append(p_name)\n",
    "\n",
    "                platform.append(p_platform)\n",
    "\n",
    "                publisher.append(p_publisher)\n",
    "\n",
    "                year.append(p_year)\n",
    "\n",
    "\n",
    "                developer.append(p_developer)\n",
    "\n",
    "                genre.append(p_genre)\n",
    "\n",
    "                meta_score.append(p_metascore)\n",
    "\n",
    "                user_score.append(p_userscore)\n",
    "\n",
    "                p_metacrit= critic_reviews(game_url)                \n",
    "                pro_critics.append(p_metacrit)\n",
    "                len_pro_critics.append(len(p_metacrit))\n",
    "                len_pro_critics.append(p_n_pro)\n",
    "\n",
    "                p_usercrit= user_reviews(game_url)\n",
    "                user_critics.append(p_usercrit)\n",
    "                len_user_critics.append(len(p_usercrit))\n",
    "                len_user_critics.append(p_n_user)\n",
    "                \n",
    "                print('Name:',p_name,' -','Platform:',p_platform,' -','Publisher:',p_publisher)\n",
    "                print('Year:',p_year,' -','Developer:',p_developer,' -','Metascore:',p_metascore,' -','Userscore:',p_userscore)\n",
    "                print('n p_crit:',len(p_metacrit),' -','n u_crit:',len(p_usercrit))\n",
    "                print('\\n')\n",
    "                sleep(5)\n",
    "                \n",
    "################################################################\n",
    "#### Fonction de parcours de chaque page du site par année  ####\n",
    "################################################################               \n",
    "                \n",
    "def metacritic_scrap(start_y,end_y):\n",
    "    # Fonction (main) du scraping de metacritic\n",
    "    \n",
    "    for annee in range(start_y,end_y):   \n",
    "\n",
    "        print('année:', annee)\n",
    "        url= url_start+str(annee)+url_end\n",
    "        headermap = {\"User-Agent\": \"Mac Firefox\"};\n",
    "        markup = requests.get(url, headers=headermap).text\n",
    "        soup= BeautifulSoup(markup,\"lxml\")\n",
    "\n",
    "        if soup.select(\"a[class=page_num]\") != [] :\n",
    "            pages= int(soup.select(\"a[class=page_num]\")[-1].text)\n",
    "        else: pages=1\n",
    "\n",
    "        start_page= 0\n",
    "        start_title= 0\n",
    "        scrap_page_title(url,start_page,start_title,pages)\n",
    "\n",
    "        columns = {\n",
    "        'Name': gname,\n",
    "        'Platform': platform,\n",
    "        'Year': year,\n",
    "        'Genre': genre,\n",
    "        'Critic_Score': meta_score,\n",
    "        'User_Score': user_score,\n",
    "        'Publisher': publisher,\n",
    "        'Developer': developer,\n",
    "        'N_proreviews': len_pro_critics,\n",
    "        'N_usereviews': len_user_critics\n",
    "        }\n",
    "    \n",
    "        df = pd.DataFrame(columns)\n",
    "        df = df[['Name','Platform','Year','Genre','Critic_Score','User_Score',\n",
    "             'Publisher','Developer','N_proreviews','N_usereviews']]\n",
    "        \n",
    "        # On génere un csv pour les jeux de l'année parcourue\n",
    "        file_name='metacrit_'+str(annee)+'.csv'\n",
    "\n",
    "        df.to_csv(file_name, sep=\",\", encoding='utf-8', index=False)\n",
    "\n",
    "        df_pro= pd.DataFrame({}, columns= ['Name', 'Platform','Genre','Date_crit', 'Name_crit','Score_crit','Comment_crit'])\n",
    "\n",
    "        # On génere un csv pour les critiques pro des jeux de l'année parcourue\n",
    "        k=0\n",
    "        for i in range (0,len(gname)):\n",
    "            for j in range(0,len(pro_critics[i])):\n",
    "                df_pro.loc[k]= [gname[i],platform[i],genre[i], pro_critics[i][j][0], pro_critics[i][j][1], pro_critics[i][j][2], pro_critics[i][j][3]]\n",
    "                k+=1\n",
    "            k+=1\n",
    "\n",
    "        file_name= 'metareview_'+str(annee)+'.csv'\n",
    "        df_pro.to_csv(file_name, sep=\",\", encoding='utf-8', index=False)\n",
    "\n",
    "        df_user= pd.DataFrame({}, columns= ['Name', 'Platform','Genre','Date_crit', 'Name_crit','Score_crit','Comment_crit'])\n",
    "\n",
    "        # On génere un csv pour les critiques utilisateurs des jeux de l'année parcourue\n",
    "        k=0\n",
    "        for i in range (0,len(gname)):\n",
    "            for j in range(0,len(user_critics[i])):\n",
    "                df_user.loc[k]= [gname[i],platform[i],genre[i], user_critics[i][j][0], user_critics[i][j][1], user_critics[i][j][2], user_critics[i][j][3]]\n",
    "                k+=1\n",
    "            k+=1\n",
    "\n",
    "        file_name= 'usereview_'+str(annee)+'.csv'\n",
    "        df_user.to_csv(file_name, sep=\",\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98094b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# début de l'url du site trié par metascore et par année\n",
    "url_start = 'https://www.metacritic.com/browse/games/score/metascore/year/all/filtered?year_selected=' \n",
    "# fin de l'url\n",
    "url_end= '&distribution=&sort=desc&view=detailed&' \n",
    "\n",
    "# Liste contenant les liens 'cassés'\n",
    "broken_link= ['https://www.metacritic.com/game/pc/agatha-christie-the-abc-murders',\n",
    "              'https://www.metacritic.com/game/3ds/3d-fantasy-zone-ii-w',\n",
    "              'https://www.metacritic.com/game/pc/no-mans-land',\n",
    "              'https://www.metacritic.com/game/playstation-2/pinball-hall-of-fame-the-gottlieb-collection',\n",
    "              'https://www.metacritic.com/game/pc/the-experiment',\n",
    "              'https://www.metacritic.com/game/ds/naruto-shippuden-shinobi-rumble',\n",
    "              'https://www.metacritic.com/game/pc/pid',\n",
    "              'https://www.metacritic.com/game/pc/memento-mori-2',\n",
    "              'https://www.metacritic.com/game/pc/yatagarasu-attack-on-cataclysm',\n",
    "              'https://www.metacritic.com/game/pc/nightcry',\n",
    "              'https://www.metacritic.com/game/xbox-one/voodoo-vince-remastered',\n",
    "              'https://www.metacritic.com/game/switch/the-longing',\n",
    "              'https://www.metacritic.com/game/pc/el-shaddai-ascension-of-the-metatron',\n",
    "             'https://www.metacritic.com/game/pc/area-51']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDateTime = datetime.datetime.now()\n",
    "date = currentDateTime.date()\n",
    "current_year = date.strftime(\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76feeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On parcours les années de 1995 à l'année actuelle si les fichiers csv n'existe pas on scape les données\n",
    "\n",
    "gname = []\n",
    "platform = []\n",
    "year = []\n",
    "genre = []\n",
    "meta_score = []\n",
    "user_score= []\n",
    "publisher = []\n",
    "developer = []\n",
    "pro_critics = []\n",
    "len_pro_critics= []\n",
    "user_critics= []\n",
    "len_user_critics= []\n",
    "\n",
    "\n",
    "for i in range(1995, int(current_year)+1):\n",
    "    metacrit_year= './metacrit_'+str(i)+'.csv'\n",
    "    metareview_year= 'metareview_'+str(i)+'.csv'\n",
    "    usereview_year= 'usereview_'+str(i)+'.csv'\n",
    "    if (os.path.exists(metacrit_year) & os.path.exists(metareview_year) & os.path.exists(usereview_year)) == False :\n",
    "        metacritic_scrap(i,i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "metacritic= pd.DataFrame()\n",
    "for i in range(1995,2023): # Pas de données avant 1995\n",
    "    filename= 'metacrit_'+str(i)+'.csv'\n",
    "    df= pd.read_csv(filename)\n",
    "    metacritic= pd.concat([metacritic,df],ignore_index=True)\n",
    "metacritic.head()\n",
    "\n",
    "# On renomme les colonnes du dataframe\n",
    "metacritic= metacritic.rename(columns={'Year': 'Release_date', 'Critic_Score' : 'Meta_Critic_Score','User_Score':'Meta_User_Score',\n",
    "                           'N_proreviews': 'N_Critic_Reviews', 'N_usereviews': 'N_User_Reviews' })\n",
    "\n",
    "# On supprime les doublons\n",
    "metacritic= metacritic.drop_duplicates(keep='first')\n",
    "\n",
    "# Enregistrement du dataset nettoyé dans metacritic_clean.csv\n",
    "metacritic.to_csv('metacritic.csv', sep=\",\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
